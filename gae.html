<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Generative AI and Epistemology</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="description" content="Generative AI and Epistemology" />

  <link rel="stylesheet" href="css/style.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css?family=Lato:100,300,400,700,900&display=swap" rel="stylesheet">
</head>
<body>
	<nav class="floating-nav">
		<a href="#cmd">Command Line AI Interface</a>
		<a href="#web">Web AI Assistant</a>
		<a href="#reflection">Reflection</a>
        <a href="index.html">Nathan Zhang</a>
	  </nav>
<main class="stack">
    <section class="panel panel-full is-visible" id="cmd">

        <div class="panel-content">
            <h1>Generative AI and Epistemology</h1>
            <h3>This page shows my command line AI interface and my web AI assistant and reflects on how our various readings
                and discussions shaped my understanding of knowledge as it pertains to generative AI.
            </h3>
                <div class="cards" style="margin-top: 2rem;">
                    <article class="card">
                        <h2>Command Line AI Interface</h2>
                        <h3>Example with prompt: </h3>
                        <img
                            src="images/cl.png"
                            alt="image of command line AI interface with an example prompt and response"
                            style="width: 100%; height: auto; border-radius: 12px; border: 1px solid #e5e7eb;"
                        >
                        <h3>Design justification</h3>
                        <p>
                        I wanted my AI assistant to be less wordy and have fewer emojis, so I used a system instruction of "Business-like and concise." I included 
                        a polite greeting and instructions on how to exit (typing 'exit').
                        </p>    
                    </article>
                </div>
  

                <div class="cards" style="margin-top: 2rem;">
                    <article class="card" id="web">
                        <h2>Web AI Assistant</h2>
                        <p>
                        Example with prompt:
                        <img
                            src="images/webai.png"
                            alt="image of web ai interface with an example prompt and response"
                            style="width: 100%; height: auto; border-radius: 12px; border: 1px solid #e5e7eb;"
                        >
                        </p>
                        <h3>Design justification</h3>
                        <p>
                        I used the flask form because it would be more user friendly than the URL route. This way, users can use a familiar
                        form that is immediately visible to type their prompts. I also included instructions on how to use the form in case 
                        it isn't clear. I chose to show the prompt and response so users can check what they typed to produce the response.
                        </p>
                        
                    </article>
                </div>
    
                <div class="cards" style="margin-top: 2rem; margin-bottom: 2rem;">
                    <article class="card" id="reflection">
                        <h2>Reflections on Knowledge as it Pertains to Generative AI</h2>

                        <p>
                            The term "knowledge" is used in various contexts and can mean different things. For example, some might refer to a collection of books as a store of 
                            knowledge. In our class, The Why of Everyday Technologies, we define knowledge as justified true beliefs. This definition would lead me to consider a
                            collection of books as a store of information rather than knowledge for the reason that beliefs imply some sort of consciousness. However, consciousness
                            is also a term with distinct meanings in different contexts. For example, Joe mentioned that humans are unconscious when asleep. For the sake of clarity, we'll assume the 
                            term consciousness refers to sentience or sapience. I don't believe that large language models can be considered sentient, and in our reading Modern-Day Oracles or Bullshit machines,
                            I learned that generative AI is designed to mimic human behavior through anthropoglossic features (Bergstrom & West, 2018). In my mind, this makes it seem like any human characteristics we see in a large language model
                            are simply the result of something akin to hard-coding. However, one of the discussion 
                            questions, "Does the underlying system of computation (binary, boolean) affect the acquisition of knowledge?" made me reconsider the idea of consciousness. We don't consider large language
                            models to be conscious because they are simply performing binary calculations that associate some grouping of words with others. Humans also associate words with definitions
                            and our own experiences. In our reading, The Allegory of the Cave (in Plato’s Republic), we see that experiences can shape knowledge and perception. This made me think that, in a sense, we draw from our own 
                            collection of experiences in the same way that large language models utilize their vast stores of data; it could be the contents that make humans more "conscious" than large language models (The Allegory of the Cave (in Plato’s Republic), 2024).
                            We may be inclined to say that our consciousness is more complex, but perhaps if we could provide generative AI with the ability to store experiences and sensory data in the way 
                            that we do, even models of the complexity that we have now could be considered conscious. This brings me back to the question "does the underlying system of computation affect the acquisition of knowledge"
                            and I would say yes, it does. Humans can be considered biological hardware that collects knowledge from our experiences and formulates beliefs based on these experiences.
                            I think the ability to perceive sensory inputs in the way that we do is exactly what separates us from large language models. Thus, I would say that large language models are not conscious,
                            and until we somehow find a way to allow computers to perceive reality in the way that we do, they cannot be considered conscious beings.  
                        </p>
                        <h3>Lingering Questions:</h3>
                        <p>What are the minimum requirements for consciousness?</p>
                        <p>Do animals have knowledge?</p>
                        <p></p>
                        <h3>
                            References   
                        </h3> 
                        <p>
                            Bergstrom, C., & West, J. (2018). Modern-Day Oracles or Bullshit Machines? Table of Contents. Thebullshitmachines.com. https://thebullshitmachines.com/table-of-contents/index.html                        </p>
                        <p>
                            The Allegory of the Cave (in Plato’s Republic) - PLATO - Philosophy Learning and Teaching Organization. (2024, October 8). PLATO. https://www.plato-philosophy.org/teachertoolkit/platos-allegory-of-the-cave/                        </p>
                    </article>
                </div>
            </div>
            </section>
  
  </main>